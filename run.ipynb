{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# from pillow_heif import register_heif_opener\n",
    "# from PIL import ImageFile\n",
    "\n",
    "# # Cho phép xử lý tệp bị lỗi hoặc không đầy đủ\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# # Đăng ký HEIF/HEIC opener với Pillow\n",
    "# register_heif_opener()\n",
    "\n",
    "# def convert_heic_to_png(heic_file, png_file):\n",
    "#     \"\"\"Chuyển đổi tệp HEIC sang PNG\"\"\"\n",
    "#     try:\n",
    "#         image = Image.open(heic_file)  # Mở tệp HEIC\n",
    "#         image.save(png_file, \"PNG\")   # Lưu dưới dạng PNG\n",
    "#         print(f\"Converted {heic_file} to {png_file}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to convert {heic_file}: {e}\")\n",
    "\n",
    "# # Thư mục chứa tệp HEIC\n",
    "# file_path = \"./DATA\"\n",
    "\n",
    "# # Duyệt qua tất cả các tệp trong thư mục\n",
    "# for file in os.listdir(file_path):\n",
    "#     if file.lower().endswith(\".heic\"):  # Kiểm tra đuôi tệp\n",
    "#         heic_file = os.path.join(file_path, file)\n",
    "#         png_file = os.path.splitext(heic_file)[0] + \".png\"\n",
    "\n",
    "#         # Kiểm tra nếu file PNG đã tồn tại\n",
    "#         if os.path.exists(png_file):\n",
    "#             print(f\"Skipping {file}, {png_file} already exists.\")\n",
    "#             continue\n",
    "\n",
    "#         # Chuyển đổi nếu file PNG chưa tồn tại\n",
    "#         convert_heic_to_png(heic_file, png_file)\n",
    "#     else:\n",
    "#         print(f\"Skipping {file}, not a HEIC file.\")\n",
    "\n",
    "# print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script run.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python mmocr_sam.py --inputs ./data/DataFood/IMG_3455.png --outdir ./OUTPUT --device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from underthesea import word_tokenize\n",
    "from mmocr.apis.inferencers import MMOCRInferencer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from underthesea import word_tokenize\n",
    "from mmocr.apis.inferencers import MMOCRInferencer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from underthesea import word_tokenize\n",
    "from mmocr.apis.inferencers import MMOCRInferencer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm scale ảnh\n",
    "def scale_image(image, scale_factor):\n",
    "    \"\"\"Scale image by a given factor.\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_width = int(width * scale_factor)\n",
    "    new_height = int(height * scale_factor)\n",
    "    scaled_image = cv2.resize(\n",
    "        image, (new_width, new_height), interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "    return scaled_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tính khoảng cách giữa hai bounding boxes\n",
    "def calculate_distance(box1, box2):\n",
    "    \"\"\"Calculate the Euclidean distance between the centers of two bounding boxes.\"\"\"\n",
    "    center1 = ((box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2)\n",
    "    center2 = ((box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2)\n",
    "    return np.linalg.norm(np.array(center1) - np.array(center2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm gom cụm văn bản theo dòng và khoảng cách\n",
    "def group_texts_by_proximity(detected_texts, line_threshold=20, distance_threshold=50):\n",
    "    \"\"\"\n",
    "    Group texts by proximity based on the vertical (y) alignment and horizontal distance.\n",
    "    \n",
    "    Args:\n",
    "        detected_texts: List of tuples [(text, (x_min, y_min, x_max, y_max)), ...].\n",
    "        line_threshold: Maximum y-difference to group texts into the same line.\n",
    "        distance_threshold: Maximum horizontal distance to group texts within the same line.\n",
    "        \n",
    "    Returns:\n",
    "        List of grouped texts as sentences.\n",
    "    \"\"\"\n",
    "    # Sắp xếp bounding boxes theo tọa độ y_min (dòng trên cùng trước)\n",
    "    detected_texts = sorted(detected_texts, key=lambda x: x[1][1])  # x[1][1] là y_min\n",
    "\n",
    "    lines = []\n",
    "    current_line = []\n",
    "\n",
    "    # Gom nhóm bounding boxes theo dòng\n",
    "    for text, box in detected_texts:\n",
    "        if not current_line:\n",
    "            current_line.append((text, box))\n",
    "            continue\n",
    "\n",
    "        _, prev_box = current_line[-1]\n",
    "        if abs(box[1] - prev_box[1]) < line_threshold:  # So sánh y_min của box\n",
    "            current_line.append((text, box))\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = [(text, box)]\n",
    "\n",
    "    if current_line:\n",
    "        lines.append(current_line)\n",
    "\n",
    "    # Gom cụm văn bản trong mỗi dòng\n",
    "    grouped_texts = []\n",
    "    for line in lines:\n",
    "        line = sorted(line, key=lambda x: x[1][0])  # Sắp xếp theo x_min (trái sang phải)\n",
    "        current_group = []\n",
    "        sentence = \"\"\n",
    "\n",
    "        for text, box in line:\n",
    "            if not current_group:\n",
    "                current_group.append((text, box))\n",
    "                sentence = text\n",
    "                continue\n",
    "\n",
    "            _, prev_box = current_group[-1]\n",
    "            if box[0] - prev_box[2] < distance_threshold:  # So sánh khoảng cách x_min với x_max trước đó\n",
    "                current_group.append((text, box))\n",
    "                sentence += \" \" + text\n",
    "            else:\n",
    "                grouped_texts.append(sentence)\n",
    "                current_group = [(text, box)]\n",
    "                sentence = text\n",
    "\n",
    "        if sentence:\n",
    "            grouped_texts.append(sentence)\n",
    "\n",
    "    return grouped_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xử lý chính tả\n",
    "def correct_vietnamese(text):\n",
    "    \"\"\"Correct Vietnamese text using underthesea.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xử lý ảnh OCR\n",
    "def process_image(image_path, output_dir, mmocr_inferencer, scale_factor):\n",
    "    \"\"\"Process a single image using MMOCR, group texts, and correct them.\"\"\"\n",
    "    # Đọc ảnh\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Cannot read image: {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Scale ảnh\n",
    "    scaled_image = scale_image(image, scale_factor)\n",
    "\n",
    "    # Thực hiện OCR\n",
    "    results = mmocr_inferencer(scaled_image, save_vis=True, out_dir=output_dir)\n",
    "\n",
    "    # Kiểm tra kết quả\n",
    "    if \"predictions\" not in results:\n",
    "        print(f\"No predictions found for {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Lấy kết quả văn bản và bounding boxes\n",
    "    detected_texts = []\n",
    "    for prediction in results[\"predictions\"]:\n",
    "        if \"rec_texts\" in prediction and \"det_polygons\" in prediction:\n",
    "            for text, polygon in zip(prediction[\"rec_texts\"], prediction[\"det_polygons\"]):\n",
    "                try:\n",
    "                    # Chuyển polygon thành bounding box\n",
    "                    points = [(polygon[i], polygon[i + 1]) for i in range(0, len(polygon), 2)]\n",
    "                    x_min = min(point[0] for point in points)\n",
    "                    y_min = min(point[1] for point in points)\n",
    "                    x_max = max(point[0] for point in points)\n",
    "                    y_max = max(point[1] for point in points)\n",
    "                    box = (x_min, y_min, x_max, y_max)\n",
    "                    detected_texts.append((text, box))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing polygon: {polygon}, Error: {e}\")\n",
    "\n",
    "    # Bỏ qua ảnh không có văn bản\n",
    "    if not detected_texts:\n",
    "        print(f\"No text detected in {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Gom cụm văn bản\n",
    "    grouped_texts = group_texts_by_proximity(detected_texts, line_threshold=20, distance_threshold=50)\n",
    "\n",
    "    # Sửa lỗi chính tả\n",
    "    corrected_texts = [correct_vietnamese(sentence) for sentence in grouped_texts]\n",
    "\n",
    "    # Ghi kết quả văn bản vào file txt\n",
    "    if corrected_texts:\n",
    "        output_text_file = os.path.join(\n",
    "            output_dir,\n",
    "            os.path.basename(image_path)\n",
    "            .replace(\".jpg\", \".txt\")\n",
    "            .replace(\".png\", \".txt\"),\n",
    "        )\n",
    "        with open(output_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(corrected_texts))\n",
    "        print(f\"Text saved to: {output_text_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xóa dữ liệu cũ trong thư mục OUTPUT\n",
    "def prepare_output_dir(output_dir):\n",
    "    \"\"\"Clear old output directory and create a new one.\"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm chính để xử lý tất cả ảnh trong thư mục\n",
    "def process_all_images(input_dir, output_dir, mmocr_inferencer, scale_factor):\n",
    "    \"\"\"Process all images in a directory.\"\"\"\n",
    "    prepare_output_dir(output_dir)\n",
    "    image_files = [\n",
    "        os.path.join(input_dir, f)\n",
    "        for f in os.listdir(input_dir)\n",
    "        if f.endswith((\".jpg\", \".png\"))\n",
    "    ]\n",
    "    for image_file in image_files:\n",
    "        process_image(image_file, output_dir, mmocr_inferencer, scale_factor)\n",
    "    print(\"Processing complete. Results saved in:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: checkpoints/mmocr/db_swin_mix_pretrain.pth\n",
      "11/28 23:47:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Delete `relative_position_index` and `relative_coords_table` since we always re-init these params according to the `window_size`, which might cause unwanted but unworried warnings when loading checkpoint.\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "missing keys in source state_dict: backbone.stages.0.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.0.blocks.0.attn.w_msa.relative_position_index, backbone.stages.0.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.0.blocks.1.attn.w_msa.relative_position_index, backbone.stages.1.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.1.blocks.0.attn.w_msa.relative_position_index, backbone.stages.1.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.1.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.0.attn.w_msa.relative_position_index, backbone.stages.2.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.2.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.2.attn.w_msa.relative_position_index, backbone.stages.2.blocks.3.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.3.attn.w_msa.relative_position_index, backbone.stages.2.blocks.4.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.4.attn.w_msa.relative_position_index, backbone.stages.2.blocks.5.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.5.attn.w_msa.relative_position_index, backbone.stages.2.blocks.6.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.6.attn.w_msa.relative_position_index, backbone.stages.2.blocks.7.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.7.attn.w_msa.relative_position_index, backbone.stages.2.blocks.8.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.8.attn.w_msa.relative_position_index, backbone.stages.2.blocks.9.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.9.attn.w_msa.relative_position_index, backbone.stages.2.blocks.10.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.10.attn.w_msa.relative_position_index, backbone.stages.2.blocks.11.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.11.attn.w_msa.relative_position_index, backbone.stages.2.blocks.12.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.12.attn.w_msa.relative_position_index, backbone.stages.2.blocks.13.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.13.attn.w_msa.relative_position_index, backbone.stages.2.blocks.14.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.14.attn.w_msa.relative_position_index, backbone.stages.2.blocks.15.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.15.attn.w_msa.relative_position_index, backbone.stages.2.blocks.16.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.16.attn.w_msa.relative_position_index, backbone.stages.2.blocks.17.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.17.attn.w_msa.relative_position_index, backbone.stages.3.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.3.blocks.0.attn.w_msa.relative_position_index, backbone.stages.3.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.3.blocks.1.attn.w_msa.relative_position_index\n",
      "\n",
      "Loads checkpoint by local backend from path: checkpoints/mmocr/abinet_20e_st-an_mj_20221005_012617-ead8c139.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cấu hình MMOCR Inferencer\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "mmocr_inferencer = MMOCRInferencer(\n",
    "    det=\"mmocr_dev/configs/textdet/dbnetpp/dbnetpp_swinv2_base_w16_in21k.py\",\n",
    "    det_weights=\"checkpoints/mmocr/db_swin_mix_pretrain.pth\",\n",
    "    rec=\"mmocr_dev/configs/textrecog/abinet/abinet_20e_st-an_mj.py\",\n",
    "    rec_weights=\"checkpoints/mmocr/abinet_20e_st-an_mj_20221005_012617-ead8c139.pth\",\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình thư mục và mô hình\n",
    "INPUT_DIR = \"./data/Test\"  # Thư mục chứa ảnh đầu vào\n",
    "OUTPUT_DIR = \"./OUTPUT\"  # Thư mục lưu kết quả\n",
    "SCALE_FACTOR = 1.5  # Hệ số scale ảnh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to: ./OUTPUT\\IMG_3464.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to: ./OUTPUT\\IMG_3529.txt\n",
      "Processing complete. Results saved in: ./OUTPUT\n"
     ]
    }
   ],
   "source": [
    "process_all_images(INPUT_DIR, OUTPUT_DIR, mmocr_inferencer, SCALE_FACTOR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
